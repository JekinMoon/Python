{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d4bcc0",
   "metadata": {},
   "source": [
    "## 10. Beautiful Soup\n",
    "- HTML 과 XML파일에서 데이터를 추출하기 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3124ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6d71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   크롤링 연습 페이지\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 class=\"title\">\n",
      "   Hello BeautifulSoup\n",
      "  </h1>\n",
      "  <h1 class=\"sub_title\">\n",
      "   안녕! 아름다운 수프\n",
      "  </h1>\n",
      "  <p id=\"description\">\n",
      "   이 페이지는 BeautifulSoup 학습을 위한 예제입니다.\n",
      "  </p>\n",
      "  <ul class=\"items\">\n",
      "   <li>\n",
      "    사과\n",
      "   </li>\n",
      "   <li>\n",
      "    바나나\n",
      "   </li>\n",
      "   <li>\n",
      "    체리\n",
      "   </li>\n",
      "  </ul>\n",
      "  <ul class=\"items\">\n",
      "   <li>\n",
      "    Python\n",
      "   </li>\n",
      "   <li>\n",
      "    C++\n",
      "   </li>\n",
      "   <li>\n",
      "    SQL\n",
      "   </li>\n",
      "  </ul>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "<a></a>\n",
      "<html><body><a></a></body></html>\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_data = f.read()\n",
    "\n",
    "# soup 객체 생성\n",
    "# soup = BeautifulSoup(html_data, \"html.parser\")        # 내장 파서\n",
    "soup = BeautifulSoup(html_data, \"lxml\")                # xml 일때 사용, 설치 필요\n",
    "# print(soup)\n",
    "print(soup.prettify())      # 들여쓰기 표시\n",
    "\n",
    "# 파서 차이 비교\n",
    "print(BeautifulSoup(\"<a></p>\", \"html.parser\"))          # <a></a>\n",
    "print(BeautifulSoup(\"<a></p>\", \"lxml\"))                 # <html><body><a></a></body></html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06951fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"title\">Hello BeautifulSoup</h1>\n",
      "Hello BeautifulSoup\n",
      "Hello BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "# 데이터 선택\n",
    "# find() - 첫번째 매칭 요소 선택\n",
    "# 1. 태그를 기준으로 탐색\n",
    "title_tag = soup.find(\"h1\")\n",
    "print(title_tag)\n",
    "print(title_tag.text)\n",
    "print(title_tag.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bebaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 아름다운 수프\n"
     ]
    }
   ],
   "source": [
    "# find() - 속성 조건으로 검색 가능\n",
    "result = soup.find(\"h1\", class_=\"sub_title\")\n",
    "print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6342e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"title\">Hello BeautifulSoup</h1>, <h1 class=\"sub_title\">안녕! 아름다운 수프</h1>]\n",
      "Hello BeautifulSoup\n",
      "안녕! 아름다운 수프\n"
     ]
    }
   ],
   "source": [
    "# find_all() - 모든 매칭된 요소 선택\n",
    "result = soup.find_all(\"h1\")\n",
    "print(result)\n",
    "\n",
    "for i in result:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b033fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ul class=\"items\">\n",
      "<li>사과</li>\n",
      "<li>바나나</li>\n",
      "<li>체리</li>\n",
      "</ul>, <ul class=\"items\">\n",
      "<li>Python</li>\n",
      "<li>C++</li>\n",
      "<li>SQL</li>\n",
      "</ul>]\n",
      "\n",
      "사과\n",
      "바나나\n",
      "체리\n",
      "\n",
      "\n",
      "Python\n",
      "C++\n",
      "SQL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() - 모든 매칭 요소 선택\n",
    "# CSS 선택자로 탐색\n",
    "result = soup.select(\"ul.items\")\n",
    "print(result)\n",
    "\n",
    "for i in result:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfc7847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"items\">\n",
       "<li>사과</li>\n",
       "<li>바나나</li>\n",
       "<li>체리</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select_one() - 첫번째 매칭 요소 선택\n",
    "result = soup.select_one(\"ul.items\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a19a2d",
   "metadata": {},
   "source": [
    "### Requests\n",
    "- HTTP 프로토콜을 이용하여 웹 사이트로부터 데이터를 송수신 하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "941e880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Good Goodbye\n",
      "2.ONE MORE TIME\n",
      "3.타임캡슐\n",
      "4.Blue Valentine\n",
      "5.Golden\n",
      "6.SPAGHETTI (feat. j-hope of BTS)\n",
      "7.Drowning\n",
      "8.첫 눈\n",
      "9.멸종위기사랑\n",
      "10.달리 표현할 수 없어요\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup & requests 함께 이용\n",
    "# 멜론에서 Top10의 노래 제목 받아오기\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "# print(response.status_code)             # 200: 성공/ 404:페이지 없음/ 500: 서버 에러\n",
    "# print(response.text)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "songs = soup.select(\"div.ellipsis.rank01 a\")[:10]\n",
    "\n",
    "for idx, song in enumerate(songs):\n",
    "    print(f\"{idx+1}.{song.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a4df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==오늘의 뉴스==\n"
     ]
    }
   ],
   "source": [
    "# <실습2>\n",
    "# 사용자에게 검색어를 입력 받아 검색된 뉴스의 제목과 링크 가져와 보세요\n",
    "\n",
    "word = input(\"검색어를 입력하세요.\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(f\"https://search.naver.com/search.naver?query={word}\", headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "news_list = soup.select(\"a.fender-ui_228e3bd1.moM44hE6Je7O8nL1iBI9\")            # a태그에 있는 클래스를 가져옴\n",
    "print(\"==오늘의 뉴스==\")\n",
    "# print(news_list)\n",
    "\n",
    "\n",
    "for a in news_list:\n",
    "    print(f\"{a.get_text()} / 링크: {a.get('href')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae80b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울지하철 1노조 파업 철회…임단협 결렬 후 극적 타결 : https://www.yna.co.kr/view/AKR20251212012100004?input=1195m\n",
      "\n",
      "[속보] 서울지하철 1노조 파업 안 한다…임단협 극적 타결 : https://www.hankookilbo.com/News/Read/A2025121206010005278?did=NA\n",
      "\n",
      "서울지하철 1노조 임단협 타결…총파업 철회 : https://news.jtbc.co.kr/article/NB12274900?influxDiv=NAVER\n",
      "\n",
      "서울지하철 1노조 \"파업 유보\"...사측 \"교섭 진행 중\" : https://www.ytn.co.kr/_ln/0115_202512120625177906\n",
      "\n",
      "서울지하철 1노조 임단협 결렬…오늘 첫차부터 파업 : https://news.sbs.co.kr/news/endPage.do?news_id=N1008365748&plink=ORI&cooper=NAVER\n",
      "\n",
      "철도노조 총파업 유보…대구지역 열차도 '정상화' : https://www.newsis.com/view/NISX20251211_0003436415\n",
      "\n",
      "서울 지하철 파업 철회…노사 임단협 극적 타결 : http://www.mbn.co.kr/pages/news/newsView.php?news_seq_no=5161354\n",
      "\n",
      "철도노조 파업 유보…KTX 등 열차 정상 운행(종합) : https://www.yna.co.kr/view/AKR20251211012451063?input=1195m\n",
      "\n",
      "서울지하철 1노조 임단협 타결…파업 철회 : https://www.donga.com/news/Society/article/all/20251212/132951802/2\n",
      "\n",
      "서울지하철 1노조 노사교섭 타결‥파업 철회·정상 운행 : https://imnews.imbc.com/news/2025/society/article/6784498_36718.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url =\"https://search.naver.com/search.naver?where=news&ie=utf8&sm=nws_hty&query=%ED%8C%8C%EC%97%85\"\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0\"\n",
    "}\n",
    "news = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(news.text, \"html.parser\") \n",
    "\n",
    "news1 = soup.select(\"span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1\")[:10]\n",
    "\n",
    "\n",
    "for i in news1:\n",
    "    title = i.get_text().strip()\n",
    "    link_tag = i.find_parent('a') \n",
    "    link = link_tag.get(\"href\") if link_tag else None\n",
    "    print(f\"{title} : {link}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06105ff",
   "metadata": {},
   "source": [
    "## openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42b2dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링한 자료를 엑셀로 저장\n",
    "import openpyxl \n",
    "\n",
    "# 엑셀 파일 만들기\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"test\"\n",
    "\n",
    "# 시트 만들기\n",
    "ws = wb.create_sheet(\"test\")\n",
    "\n",
    "ws[\"A1\"] = \"이름\"\n",
    "ws[\"B1\"] = \"나이\"\n",
    "\n",
    "ws[\"A2\"] = \"홍길동\"\n",
    "ws[\"B2\"] = 30\n",
    "\n",
    "wb.save(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54cca037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl \n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active   # 기본 시트를 가져옵니다\n",
    "ws.title = \"test\"\n",
    "\n",
    "ws[\"A1\"] = \"이름\"\n",
    "ws[\"B1\"] = \"나이\"\n",
    "\n",
    "ws[\"A2\"] = \"홍길동\"\n",
    "ws[\"B2\"] = 30\n",
    "\n",
    "wb.save(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e0a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl \n",
    "\n",
    "# 파일 불러오기\n",
    "wb = openpyxl.load_workbook(\"test.xlsx\")\n",
    "\n",
    "# 시트 선택\n",
    "ws = wb[\"test\"]\n",
    "\n",
    "# 여러 자료 추가\n",
    "data =[\n",
    "    [\"kim\", 20],\n",
    "    [\"lee\", 14],\n",
    "    [\"park\",34]\n",
    "]\n",
    "\n",
    "for row in data:\n",
    "    ws.append(row)\n",
    "\n",
    "wb.save(\"test.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39bcad7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# <실습3>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = requests.get(\u001b[33m\"\u001b[39m\u001b[33mhttps://finance.naver.com/marketindex/\u001b[39m\u001b[33m\"\u001b[39m, headers=\u001b[43mheaders\u001b[49m)\n\u001b[32m      3\u001b[39m soup = BeautifulSoup(response.text, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m result = soup.select(\u001b[33m\"\u001b[39m\u001b[33mdiv.market1 a.head\u001b[39m\u001b[33m\"\u001b[39m)          \n",
      "\u001b[31mNameError\u001b[39m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "# <실습3>\n",
    "response = requests.get(\"https://finance.naver.com/marketindex/\", headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "result = soup.select(\"div.market1 a.head\")          \n",
    "\n",
    "headers = {\"User-Agent\" : \"Mozilla/5.0\"}\n",
    "\n",
    "from openpyxl import Workbook\n",
    "wb = Workbook() \n",
    "ws = wb.active\n",
    "ws.title = \"환율\"\n",
    "\n",
    "# data=[]\n",
    "ws.append([\"통화\", \"환율\"])\n",
    "for a in result:\n",
    "    exchange = a.select_one(\"span.blind\").get_text().split()[1] # USD\n",
    "    value = a.select_one(\"span.value\").get_text()\n",
    "    # data.append([exchange, value]) # df 이용 저장 (to_excel)\n",
    "    ws.append([exchange, value])\n",
    "\n",
    "\n",
    "wb.save(\"test.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c51e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원 : 1,472.20\n",
      "원 : 946.17\n",
      "원 : 1,727.77\n",
      "원 : 208.76\n",
      "엔 : 154.9400\n",
      "달러 : 1.1740\n",
      "달러 : 1.3420\n",
      "하락 : 98.3400\n",
      "달러 : 57.6\n",
      "원 : 1744.69\n",
      "달러 : 4313.0\n",
      "원 : 202063.97\n",
      "엑셀 파일 생성 완료: 환율.xlsx\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from openpyxl import Workbook\n",
    "\n",
    "# url = \"https://finance.naver.com/marketindex/\"\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "# response = requests.get(url, headers=headers)\n",
    "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# exchange_list = soup.select(\"div.head_info\")\n",
    "\n",
    "# for item in exchange_list:\n",
    "#     name_tag = item.select_one(\"span.blind\")\n",
    "#     rate_tag = item.select_one(\"span.value\")\n",
    "    \n",
    "#     if name_tag and rate_tag:\n",
    "#         name = name_tag.get_text(strip=True)\n",
    "#         rate = rate_tag.get_text(strip=True)\n",
    "#         print(f\"{name} : {rate}\")\n",
    "        \n",
    "\n",
    "# wb = Workbook()\n",
    "# ws = wb.active\n",
    "# ws.title = \"환율\"\n",
    "\n",
    "# # 헤더 작성\n",
    "# ws[\"A1\"] = \"국가\"\n",
    "# ws[\"B1\"] = \"환율\"\n",
    "\n",
    "# row = 2\n",
    "\n",
    "# # 환율 항목 선택\n",
    "# # 네이버 금융 메인 페이지의 주요 환율은 div.head_info 안에 있음\n",
    "# for item in soup.select(\"div.head_info\"):\n",
    "#     name_tag = item.select_one(\"span.blind\")   # 통화명\n",
    "#     value_tag = item.select_one(\"span.value\")  # 환율 값\n",
    "    \n",
    "#     if name_tag and value_tag:\n",
    "#         name = name_tag.get_text(strip=True)\n",
    "#         value = value_tag.get_text(strip=True)\n",
    "        \n",
    "#         ws[f\"A{row}\"] = name\n",
    "#         ws[f\"B{row}\"] = value\n",
    "#         row += 1\n",
    "\n",
    "# # 엑셀 저장\n",
    "# wb.save(excel_file)\n",
    "# print(f\"엑셀 파일 생성 완료: {excel_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
