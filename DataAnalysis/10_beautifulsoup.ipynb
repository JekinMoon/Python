{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d4bcc0",
   "metadata": {},
   "source": [
    "## 10. Beautiful Soup\n",
    "- HTML 과 XML파일에서 데이터를 추출하기 위한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3124ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6d71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   크롤링 연습 페이지\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 class=\"title\">\n",
      "   Hello BeautifulSoup\n",
      "  </h1>\n",
      "  <h1 class=\"sub_title\">\n",
      "   안녕! 아름다운 수프\n",
      "  </h1>\n",
      "  <p id=\"description\">\n",
      "   이 페이지는 BeautifulSoup 학습을 위한 예제입니다.\n",
      "  </p>\n",
      "  <ul class=\"items\">\n",
      "   <li>\n",
      "    사과\n",
      "   </li>\n",
      "   <li>\n",
      "    바나나\n",
      "   </li>\n",
      "   <li>\n",
      "    체리\n",
      "   </li>\n",
      "  </ul>\n",
      "  <ul class=\"items\">\n",
      "   <li>\n",
      "    Python\n",
      "   </li>\n",
      "   <li>\n",
      "    C++\n",
      "   </li>\n",
      "   <li>\n",
      "    SQL\n",
      "   </li>\n",
      "  </ul>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "<a></a>\n",
      "<html><body><a></a></body></html>\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_data = f.read()\n",
    "\n",
    "# soup 객체 생성\n",
    "# soup = BeautifulSoup(html_data, \"html.parser\")        # 내장 파서\n",
    "soup = BeautifulSoup(html_data, \"lxml\")                # xml 일때 사용, 설치 필요\n",
    "# print(soup)\n",
    "print(soup.prettify())      # 들여쓰기 표시\n",
    "\n",
    "# 파서 차이 비교\n",
    "print(BeautifulSoup(\"<a></p>\", \"html.parser\"))          # <a></a>\n",
    "print(BeautifulSoup(\"<a></p>\", \"lxml\"))                 # <html><body><a></a></body></html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06951fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"title\">Hello BeautifulSoup</h1>\n",
      "Hello BeautifulSoup\n",
      "Hello BeautifulSoup\n"
     ]
    }
   ],
   "source": [
    "# 데이터 선택\n",
    "# find() - 첫번째 매칭 요소 선택\n",
    "# 1. 태그를 기준으로 탐색\n",
    "title_tag = soup.find(\"h1\")\n",
    "print(title_tag)\n",
    "print(title_tag.text)\n",
    "print(title_tag.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bebaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 아름다운 수프\n"
     ]
    }
   ],
   "source": [
    "# find() - 속성 조건으로 검색 가능\n",
    "result = soup.find(\"h1\", class_=\"sub_title\")\n",
    "print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6342e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"title\">Hello BeautifulSoup</h1>, <h1 class=\"sub_title\">안녕! 아름다운 수프</h1>]\n",
      "Hello BeautifulSoup\n",
      "안녕! 아름다운 수프\n"
     ]
    }
   ],
   "source": [
    "# find_all() - 모든 매칭된 요소 선택\n",
    "result = soup.find_all(\"h1\")\n",
    "print(result)\n",
    "\n",
    "for i in result:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b033fd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<ul class=\"items\">\n",
      "<li>사과</li>\n",
      "<li>바나나</li>\n",
      "<li>체리</li>\n",
      "</ul>, <ul class=\"items\">\n",
      "<li>Python</li>\n",
      "<li>C++</li>\n",
      "<li>SQL</li>\n",
      "</ul>]\n",
      "\n",
      "사과\n",
      "바나나\n",
      "체리\n",
      "\n",
      "\n",
      "Python\n",
      "C++\n",
      "SQL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() - 모든 매칭 요소 선택\n",
    "# CSS 선택자로 탐색\n",
    "result = soup.select(\"ul.items\")\n",
    "print(result)\n",
    "\n",
    "for i in result:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfc7847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"items\">\n",
       "<li>사과</li>\n",
       "<li>바나나</li>\n",
       "<li>체리</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select_one() - 첫번째 매칭 요소 선택\n",
    "result = soup.select_one(\"ul.items\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a19a2d",
   "metadata": {},
   "source": [
    "### Requests\n",
    "- HTTP 프로토콜을 이용하여 웹 사이트로부터 데이터를 송수신 하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "941e880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Good Goodbye\n",
      "2.ONE MORE TIME\n",
      "3.타임캡슐\n",
      "4.Blue Valentine\n",
      "5.SPAGHETTI (feat. j-hope of BTS)\n",
      "6.Golden\n",
      "7.Drowning\n",
      "8.멸종위기사랑\n",
      "9.첫 눈\n",
      "10.달리 표현할 수 없어요\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup & requests 함께 이용\n",
    "# 멜론에서 Top10의 노래 제목 받아오기\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "# print(response.status_code)             # 200: 성공/ 404:페이지 없음/ 500: 서버 에러\n",
    "# print(response.text)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "songs = soup.select(\"div.ellipsis.rank01 a\")[:10]\n",
    "\n",
    "for idx, song in enumerate(songs):\n",
    "    print(f\"{idx+1}.{song.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76a4df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "철도노조 파업 유보…KTX 등 열차 정상 운행 : https://www.yna.co.kr/view/AKR20251211012400063?input=1195m\n",
      "\n",
      "철도노조 총파업 유보…KTX 등 열차 정상 운행 : https://news.tf.co.kr/read/life/2271282.htm\n",
      "\n",
      "[속보]서울지하철 9호선 파업 철회…1~8호선은 막판교섭 : https://www.khan.co.kr/article/202512110503001\n",
      "\n",
      "철도노조 내일부터 무기한 파업…노사 협상 결렬 : https://news.sbs.co.kr/news/endPage.do?news_id=N1008363632&plink=ORI&cooper=NAVER\n",
      "\n",
      "코레일 노조 \"총파업 유보\"...모든 열차 정상운행 : https://www.ytn.co.kr/_ln/0102_202512110250231896\n",
      "\n",
      "철도노조 내일 오전 9시부터 무기한 파업…노사 협상 결렬(종합) : https://www.yna.co.kr/view/AKR20251210126851063?input=1195m\n",
      "\n",
      "[속보] 코레일 \"철도노조 파업 유보…11일 전 열차 정상 운행\" : https://www.hankyung.com/article/202512116248i\n",
      "\n",
      "코레일 노사 '성과급 정상화' 잠정합의…파업 철회(종합) : https://www.newsis.com/view/NISX20251211_0003436478\n",
      "\n",
      "서울교통公 9호선 노사 진통 끝 합의…총파업 일단락 : http://www.edaily.co.kr/news/newspath.asp?newsid=01567846642397536\n",
      "\n",
      "서울지하철 노사 임단협 막판 교섭…결렬시 내일부터 파업 : https://www.yna.co.kr/view/AKR20251210163000004?input=1195m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <실습2>\n",
    "# 사용자에게 검색어를 입력 받아 검색된 뉴스의 제목과 링크 가져와 보세요\n",
    "url =\"https://search.naver.com/search.naver?where=news&ie=utf8&sm=nws_hty&query=%ED%8C%8C%EC%97%85\"\n",
    "headers = {\n",
    "    \"User-Agent\" : \"Mozilla/5.0\"\n",
    "}\n",
    "news = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(news.text, \"lxml\") \n",
    "\n",
    "news1 = soup.select(\"span.sds-comps-text.sds-comps-text-ellipsis.sds-comps-text-ellipsis-1.sds-comps-text-type-headline1\")[:10]\n",
    "\n",
    "\n",
    "for i in news1:\n",
    "    title = i.get_text().strip()\n",
    "    link_tag = i.find_parent('a') \n",
    "    link = link_tag.get(\"href\") if link_tag else None\n",
    "    print(f\"{title} : {link}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
